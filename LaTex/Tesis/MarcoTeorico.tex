\chapter{Antecedentes teóricos}

En 2016 Colombia ocupó el puesto catorce entre doscientos cincuenta y un paises por el tamaño de su población carcelaria (120 914 hbts.) y el cincuenta y uno según la tasa de encarcelamiento (240 por cada 100.000 hbts). Tasa que pasó de 51,5 en  el año 2000 a 240 por cada 100.000 hbts en 2016. Con una ocupación del 154\% de las plazas disponibles, resulta relevante contar con proyecciones de la población carcelaria en el corto, mediano y largo plazo.\cite{InstitueforCriminalPolicyResearch2016}

\section{Proyecciones de población}

``Una estimación poblacional consiste en determinar el tamaño o las características de una población, para el momento actual o para uno anterior, en ausencia de información. Cuando se realizan un conjunto de supuestos sobre el comportamiento de los vitales hacia el futuro, hablamos de proyección, y cuando se escoge un escenario como el más probable, hablamos de pronóstico'' \cite {Swanson2012}.

Para incluir la incertidumbre en las proyecciones de población Lee(1994) considera los siguientes métodos \cite {Lee1994}: 

\begin{itemize}
	\item El enfoque de escenarios alto, medio y bajo: Asume comportamientos fijos para la fertilidad, la mortalidad y las migraciones durante el período de proyección, basado en algunos supuestos \cite {Lee1994}.
	\item Análisis estocásticos
	\begin{itemize}
		\item Análisis ex-post: consiste en evaluar el error de pronóstico en proyecciones anteriores y aplicarlo a las nuevas proyecciones \cite {Lee1994}.
		\item Simulación estocástica: Permite hacer proyecciones de población, al asignar una distribución de probabilidad a las tasas vitales (mortalidad, natalidad, migraciones) \cite {Lee1994}.
		\item Modelos estocásticos de la tasa de crecimiento: Consiste en estimar la tasa de crecimiento del total de la población; aunque permite estimar intervalos de confianza, no permite separar la proyección de las tasas vitales, ni de las franjas etarias \cite {Lee1994}.
		\item Matrices de Leslie con modelos estimados para las tasas vitales:  Al usar matrices de Leslie se estima la población por rangos etarios para un instante i, y se calcula la población en el instante i + 1 aplicando la natalidad y la mortalidad proyectadas para el período i. Puesto que las series de población carcelaria no se publican separadas por edad, no se abordará esta técnica.
	\end{itemize}
\end{itemize}

\subsection{Proyección de poblaciones pequeñas}

La proyección de áreas pequeñas es entendida como la proyección a un nivel geográfico menor al nacional. Estas proyecciones pueden incluir, departamentos, ciudades o poblaciones especiales \cite {Swanson2012}.
``Una población especial es un grupo poblacional que se encuentra restringido a un área por una medida administrativa o legislativa. Dentro de los grupos usualmente considerados se encuentran las prisiones, universidades, hospitales e instituciones militares''.
Este tipo de población puede tener una estructura etaria y de sexo, y unos vitales diferentes al resto de la población; además no suelen envejecer en el mismo lugar, lo que permite mantener una estructura etaria que no varía a través del tiempo. \cite {Swanson2012}.

\subsection{Aplicaciones nacionales e internacionales}

Las proyecciones de poblaciones carcelarias oficiales analizadas corresponden, en buena parte, a los métodos expuestos en los capítulos anteriores:  Proyecciones por escenarios, proyección de la tasa de crecimiento, modelos ARIMA para las tasas de ingreso y salida.

En Colombia (CONPES 3828) se proyectó la población carcelaria usando la tasa media de crecimiento anual (1993-2014) \cite {DepartamentoNacionaldePlaneacion2015}. Esta proyección no tiene en cuenta la incertidumbre asociada con las variaciones aleatorias en las tasas, ni las asociadas a cambios estructurales, ni que son una población especial, con patrones muy distintos a los del resto de la población \cite{Swanson2012} .

El Reino Unido hasta 2015 realizaba una proyección por escenarios (alto, medio y bajo), año en el cual cambió a un modelo de proyección de la media y su incertidumbre. La incertidumbre se incluyó a través de un análisis ex-post, de la  desviación de la proyección en años anteriores \cite {Justice2014}.

El departamento de Justicia de los Estados Unidos realizó estimaciones de la población carcelaria por estado para el período 2013-2014. Las estimaciones parten del censo de prisiones 1993-2014. Estas proyecciones se puede enmarcar dentro de las proyecciones de áreas pequeñas \cite {Minton2015}.

El bureau de estadísticas e investigación del crimen en Australia proyecta las tasas de arresto y sentencia usando modelos ARIMA;  a partir de estas tasas proyecta la población carcelaria. Estas proyecciones incluyen un período de validación de tres años. Los resultados mostraban que la serie real se encuentra dentro de los intervalos de confianza de la proyección, cercano a la proyección de la media \cite{Wan1}.

Blummstein desarrolla un método de proyección basado en los componentes demográficos, tasas específicas de arresto por delito y reincidencias, a partir de estos datos proyecta el tamaño y la composición de las poblaciones \cite{Blumstein1980a}.

Con los datos libres disponibles en Colombia no se podría utilizar el enfoque ARIMA ni el método de Blummstein, pues las tasas de encarcelamiento y sentencia no se publican. La tesis busca proponer un método de proyección, para situaciones donde no se cuenta con el registro de los vitales o su equivalente en la población analizada, en este escenario resulta conveniente usar las series ARIMA sobre la series de población sindicada y población sindicada, y los modelos estado-espacio para la serie bivariada. Se proponen los métodos demográficos de estimación para áreas pequeñas Censal-ratio y Ratio-Correlation, y su contraste con las metodologías ARIMA y Estado-Espacio que a diferencia de los métodos demográficos mencionados incorporan una medida de incertidumbre.

\section{Series de tiempo}

Una serie de tiempo es un conjunto de observaciones ${y_t}$ asociadas a un instante de tiempo t. Es usual referirse como series de tiempo, tanto a las realizaciones ${y_t}$ como a las variables aleatorias ${Y_t}$ que las generan.\cite{Brockwell2011} 

En una regresión lineal clásica, una variable $Y$ es explicada o predicha en función de un conjunto de n variables $X$. La diferencia entre el valor observado en el instante t $y_t$ y el valor predicho se suponen provenientes de un proceso aleatorio con media cero. \cite{Commandeur2007}

\begin{equation}\label{eq:regresion}
y_t = \beta_{0} + \beta_{1} x_{t1} + \beta_{2} x_{t2} ... \beta_{n} x_{tn}  + w_t
\end{equation}

También es posible representar la observación $y_t$ en función de las observaciones anteriores como:

\begin{equation}\label{eq:serierecursiva}
y_t = \beta_{0} + \beta_{1} y_1 + \beta_{2} y_2 + ... + \beta_{t-1} y_{t-1} +  w_t
\end{equation}

La variable aleatoria $Y$ en el instante i, es una combinación lineal de los valores observados de la variable aleatoria, más un error aleatorio, donde $\{\epsilon_1, \epsilon_2, ...\}$ son independientes e idénticamente distribuidas (i.i.d.). \cite{Shumway} Este supuesto no suele cumplirse, razón por la cual el análisis de series de tiempo se ha desarrollado como un área particular de la estadística \cite{Brockwell2011}.

Se define un proceso como ruido blanco a la colección de variables aleatorias $z_t$, no correlacionadas, con media $0$ y varianza $\sigma^2_w$. \cite{Shumway}

Bajo un modelo de $k$ parámetros, el estimador máximo verosímil de la varianza es: 

\begin{equation}\label{varianza}
\hat {\sigma}^2_k = \frac{SSE(k)}{n} = \frac{1}{n} \sum_{t = 1}^{n}  (y_t - \hat{y_t})
\end{equation}

Para seleccionar el modelo que mejor explique los datos observados se recurre a los criterios AIC (Criterio de Información de Aikake) y BIC (Criterio de Información Bayesiano), que penalizan los modelos con mayor cantidad de parámetros y están definidos como: 

\begin{equation}\label{AIC}
AIC = ln (\hat{\sigma_k}^2) + \frac{n+2k}{n}
\end{equation}

\begin{equation}\label{AIC}
BIC = ln (\hat{\sigma_k}^2) + \frac{k (ln(n))}{n}
\end{equation}

donde $k$ es la cantidad de parámetros en el modelo y $n$ es la cantidad de observaciones \cite{Tsay2002}.  Tanto en el AIC como en el BIC se selecciona el modelo de $k$ parámetros con menor valor.  

Para medir la dependencia entre las observaciones recurrimos a la función de autocovarianca y a la función de autocorrelación. La función de autocovarianza se define como \cite{Shumway}: 

\begin{equation}\label{covarianza}
\gamma(s,t) = cov(y_{s}, y_{t}) = E[(y_{s} - \mu_{s})(y_{t} - \mu_{t})]
\end{equation}

para todo s y t, donde $\mu = EY_t$ y k representa la cantidad de pasos entre dos observaciones. La autocovarianza mide la dependencia lineal entre dos puntos en la misma serie en diferentes instantes. \cite{Shumway}. 

La función de autocorrelación (ACF) se define como \cite{Brockwell2011}: 

\begin{equation}\label{correlacion}
\rho (s,t) = \dfrac{\gamma(s,t)}{\sqrt{\gamma(s,s) \gamma(t,t)}} 
\end{equation}


Para poder realizar predicciones sobre el estado futuro de una serie de tiempo, es necesario suponer que el comportamiento de la serie es estable a través del tiempo, incluso con un componente aleatorio. La estacionaridad es el concepto que permite articular esta necesidad. Estamos ante un \textbf{proceso estacionario} cuando las variables $\{X_1,..., X_k\}$ tienen la misma distribución conjunta que $\{X_{h+1},...,X_{h+k}\}$, para todos los enteros $h$ y $k$ \cite{Brockwell2011}.

La estacionaridad débil se presenta cuando $E[Y_t]$ y $E[Y_t,Y_{t+h}]$ son independientes de $t$, es decir: i) Presenta media $\mu$ constante e independiente de $t$ ii) La función de autocovarianza $\gamma(h, h+k)$ depende solamente de la cantidad de pasos que separa las observaciones ($k$). En adelante, al referirse a estacionaridad, se tratará de estacionaridad debil, a menos que se indique lo contrario.\cite{Shumway}

Cuando la serie no tiene un comportamiento estacionario se puede recurrir a diferenciar la serie, este proceso al ser de uso frecuente tiene su propia notación. La diferencia $\bigtriangledown y_t = y_t - y_{t-1}$ se conoce como diferencia de primer orden. La diferencia de segundo orden se define como $\bigtriangledown^2 y_t = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2} )$.

El operador de rezago B se define como \cite{Shumway}:

\begin{equation}\label{backshift}
By_t = y_{t_1} 
\end{equation}

tal que: 
\begin{equation}\label{backshift2}
\bigtriangledown y_t = (1-B) y_t 
\end{equation}

La notación se puede extender de forma que: 

\begin{equation}\label{backshift3}
\bigtriangledown^d y_t = (1-B)^d y_t 
\end{equation}

La función de auto-correlación (ACF) de una serie de tiempo estacionaria  se define como \cite{Shumway}: 

\begin{equation}\label{autocorrelacion}
\rho (h) = \frac{\gamma (h)}{\gamma(0)}
\end{equation}

donde $\gamma (h) = cov(y_{t+h},y_t) = E[(y_{t-h} - \mu)(y_t- \mu)]$ y $\bar{y}$ denota la media del proceso.


Para estimar la ACF utilizamos la función de autocorrelación muestral definida como \cite{Shumway}: 

\begin{equation}\label{autocorrelacionmuestral}
\hat{\rho (h)} = \frac{\hat{\gamma} (h)}{\hat{\gamma} (0)} = \frac{\sum_{t = 1}^{n - h}(y_{t+h}-\bar{y})(y_t - \bar{y})}{\sum_{t = 1}^{n}(y_t-\bar{y})^2}
\end{equation}
con $ h = 0,1, ..., n-1$.

Si tenemos las variables aleatorias $X,Y,Z$ y queremos medir la correlación entre las variables $Z$ y $X$, descontando el efecto de la variable Y, recurrimos a la función de correlación parcial $\rho_{XY|Z}$, que se calcula como \cite{Shumway}: 

\begin{equation}\label{correlacionparcial}
\rho_{XY|Z} = corr\{ X-\hat{X},Y-\hat{Y}\}
\end{equation}

Al adaptar este concepto a series de tiempo tenemos que la función de autocorrelación parcial de un proceso estacionario $y_t$, $\alpha_{hh}$, con $h = 1, 2, ... , t$ se define como \cite{Shumway}: 

\begin{equation}\label{correlacionparcialestacionario1}
\alpha_{11} = corr(y_1,y_0)  = \rho(1)
\end{equation}

y 

\begin{equation}\label{correlacionparcialestacionario2}
\alpha_{hh} = corr(y_h - \hat{y_h},y_0 - \hat{y_0}), h \geq 2 
\end{equation}

donde $\hat{y_h}$ es la regresión de $y_h$ en $\{y_1,y_2, ..., y_{h-1}\}$  y $\hat{y_0}$ es la regresión de $y_0$ en $\{y_1,y_2, ..., y_{h-1}\}$.

En un proceso estacionario $x_t $ la PACF es la correlación entre $ x_t,x_{t+h} $ una vez controlado por los elementos entre ellos $ \{x_{t-h+1}, x_{t-h+2},..., x_{t-1} \} $ \cite{Shumway}.

\subsection{Modelos SARIMA}

Box \& Jenkins proponen una aproximación iterativa de  cuatro etapas para la selección de un modelo \cite{Box2013}: 

\begin{enumerate}
	\item Selección de una clase de modelos, con base en la teoría y la práctica.
	\item Identificación del modelo, donde se seleccionan un conjunto de parámetros que permitan explicar el sistema con parsimonia. 
	\item Estimación de parámetros. 
	\item Chequeo diagnóstico, para detectar fallas en el ajuste. Si se detectan fallas en el ajuste, se regresa al segundo paso. 
\end{enumerate}

Shumway en \cite{Shumway} plantea que un proceso es auto-regresivo de orden p o AR(p) si:

\begin{equation}\label{ar}
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} ... + \phi_p y_{t-p} + w_t
\end{equation}

Donde $y_t$ es estacionaria con $\mu = 0$ ;  $ \phi_1,  \phi_2,...  ,\phi_p$ son constantes  ($\phi_p \neq 0$) y los $w_t$ son independientes e idénticamente distribuidos. Cuando la $\mu \neq 0$

\begin{equation}\label{ar1}
y_t - \mu  = \phi_1 (y_{t-1} - \mu) + \phi_2 (y_{t-2}-\mu) ... + \phi_p (y_{t-p}-\mu) + w_t
\end{equation}

Que se puede representar como:

\begin{equation}\label{ar2}
y_t  = \mu(1 - \phi_1 ... - \phi_p) + \phi_1 (y_{t-1}) + \phi_2 (y_{t-2}) ... + \phi_p (y_{t-p}) + w_t
\end{equation}

También es posible utilizar el operador B para representar la serie \ref{ar} como:

\begin{equation}\label{ar3}
y_t  - \phi_1 (y_{t-1}) - \phi_2 (y_{t-2}) ... - \phi_p (y_{t-p}) =  w_t
\end{equation} 

\begin{equation}\label{ar4}
(1 - \phi_1 B - \phi_2 B^2 ... - \phi_p B^p)y_t =  w_t
\end{equation} 

o en una notación simplificada: 

\begin{equation}\label{ar5}
\phi(B)y_t =  w_t
\end{equation} 

Se considera un proceso de promedio móvil de orden q o MA(p) si:

\begin{equation}\label{ma}
y_t = \theta_1 w_{t-1} + \theta_2 w_{t-2} ... \theta_q w_{t-q} + w_t
\end{equation}

Donde $ \theta_1,  \theta_2,...  ,\theta_q$ son parámetros  ($\phi_p \neq 0$) y los $w_t$ son independientes e idénticamente distribuidos y $y_t$ tiene media cero. A diferencia de los procesos auto-regresivos, los procesos de promedio movil son estacionarios sin importar el valor de $\theta_1,  \theta_2,...  ,\theta_q$ \cite{Shumway}. Este proceso también se puede representar usando el operador de rezago al ponerlo en la forma: 

\begin{equation}\label{ma1}
y_t = (\theta_1 B + \theta_2 B^2 ... \theta_q B^q + 1) w_t
\end{equation}

o en forma simplificada: 

\begin{equation}\label{ma2}
y_t = \theta (B) w_t
\end{equation}

Un proceso se considera ARMA(p,q) si $y_t$ es estacionaria y  \cite{Shumway} : 

\begin{equation}\label{arma1}
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \theta_1 w_{t-1} + \theta_2 w_{t-2} ... \theta_q w_{t-q} + w_t
\end{equation}

Con $E(y_t) = \mu = 0$. Cuando $(\mu \neq 0)$ se puede representar como: 

\begin{equation}\label{arma2}
y_t  = \mu(1 - \phi_1 ... - \phi_p) + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \theta_1 w_{t-1} + \theta_2 w_{t-2} ... \theta_q w_{t-q} + w_t
\end{equation}

A este modelo se le conoce como ARMA(p,q). Usando las ecuaciones \ref{ar5} y  \ref{ma2} se puede representar como: 

\begin{equation}\label{arma3}
\phi(B)y_t  =  \theta (B) w_t
\end{equation}

Una técnica común para tratar con series no estacionarias consiste en diferenciarlas hasta que la serie obtenida sea ARMA(p,q) \cite{Brockwell2011}. Si la serie se diferencia $d$ veces el proceso se nota como ARIMA(p,d,q).

Un proceso ARIMA (i,1,j) resulta al considerar una serie de la forma \cite{Shumway}: 

\begin{equation}\label{ARIMA}
y_t = \alpha +  y_{t-1}
\end{equation}

Tal que el proceso $y_t - y_{t-1}$ es un proceso ARMA(p,q).

Los criterios para identificar el orden de un ARMA son \cite{Boland2011}: 

\begin{itemize}
	\item Cuando la función de autocorrelación (ACF) se reduce progresivamente, y  la función de autocorrelación parcial (PACF) no tiene picos en lags luego de p, es un proceso autoregresivo de orden p  AR(p) 
	\item Cuando la función de autocorrelación tiene un pico en el lag q, y la función de autocorrelación parcial se reduce progresivamente, es un proceso de media mmovil de orden q MA(q)
	\item Si ambas funciones se reducen gradualmente, se trata de un ARMA (p,q)
\end{itemize}

Cuando las series presentan un comportamiento estacional se recurre al proceso de diferenciación estacional de la forma: \cite{Tsay2002}

\begin{equation}\label{ARIMA}
\bigtriangledown_s y_t = y_t - y_{t-s} = (1 - B^s) y_t
\end{equation}

Un proceso autoregresivo de orden $P$, promedio móvil de orden $Q$ estacional $ARMA(P,Q)_s$, donde s corresponde a la periodicidad de la serie se define como \cite{Shumway}: 

\begin{equation}\label{sarma1}
\Phi_P  (B^s) y_t  =  \Theta_Q (B^s) w_t
\end{equation}

donde $\Phi_P (B^s) = 1- \Phi_1 B^s - \Phi_2 B^{2s}- ... - \Phi_P B^{Ps}$ se denomina operador estacional autoregresivo de orden P y $\Theta_Q (B^s) = 1 + \Theta_1 B^s + \Theta_2 B^{2s} + ... + \Theta_Q B^{Qs}$ se denomina operador estacional promedio movil de orden Q \cite{Shumway}.

Un proceso SARIMA(p,d,q)(P,D,Q) se define como: 

\begin{equation}\label{sarma1}
\Phi_P  (B^s) \phi_p (B^s) \bigtriangledown^D \bigtriangledown^d y_t  =  \Theta_Q (B^s) \theta_q (B) w_t
\end{equation}

donde $w_t$ es un proceso gausiano de ruido blanco.

El paquete $astsa$ se ha utilizado para estimar los modelos ARIMA en el capitulo 3. La función $sarima$ del paquete $astsa$ es un manejador de la función $arima$ del paquete base. \cite{Stoffer2014}. La función $arima$ utiliza la función $optim$ para encontrar estimadores de máxima verosimilitud.\cite{RCoreTeam2017}.

El paquete $astsa$ se ha utilizado para realizar los pronósticos en el capitulo 3. La función $sarima.for$ del paquete $astsa$ es un manejador de la función $predict.Arima$ del paquete base. \cite{Stoffer2014}. La función $predict.Arima$ utiliza la función $KalmanForecast$. En la siguiente sección se hablará en detalle del filtro de Kalman. \cite{RCoreTeam2017}

Una vez se ha realizado la estimación del modelo, se analiza el comportamiento del error. Para descartar autocorrelación en el error se utiliza la estadística Q de Ljung, definida como:

\begin{equation}\label{ljung}
Q = n(n+2) \sum_{n=1}^{H} \frac{ \hat {\rho_{e}}^2(h)}{n-1}
\end{equation}

donde $H$ es el rezago analizado, $\hat{\rho_{e}}(h)$  es la autocorrelación estimada del error en el rezago h. La hipótesis nula es que los errores son no correlacionados. Bajo la hipótesis nula $Q \sim \chi_{H}^2$. \cite{Ljung1978} \cite{Shumway}

\subsection{Modelos Estado-Espacio}

Los modelos Estado-Espacio en su forma más simple, emplea un vector autoregresivo de la forma \cite{Shumway2006}: 

\begin{equation}\label{StateSpace1}
\boldsymbol{y_t} = \Phi \boldsymbol{y_{t-1}} + \boldsymbol{w_t}
\end{equation}

donde $\boldsymbol{y_t}$ es un vector de $p$ componentes. $ \boldsymbol w_t$ es un vector de $p \times 1$ variables aleatorias independientes e idénticamente distribuidas normalmente con media cero y matriz de covarianzas $ \boldsymbol Q$. Asumimos que el proceso es normal con un vector $ \boldsymbol {y_0}$ con media $ \boldsymbol {\mu_0}$ y matriz de covarianzas $ \boldsymbol \sum$. \cite{Shumway2006}


\begin{equation}\label{StateSpace2}
\boldsymbol{z_t} = A_t \boldsymbol{y_{t}} + \boldsymbol{v_t}
\end{equation}

donde $A_t$ es de tamaño $q \times p$. Se suponen $\boldsymbol {w_t}$ y $\boldsymbol {v_t}$ no correlacionados.

Variables exógenas o inputs fijos pueden ingresar en los estados o las observaciones. En este caso tendremos un vector $u_t$ de tamaño $r \times 1$: 

\begin{equation}\label{StateSpace6}
\boldsymbol{y_t} = \Phi \boldsymbol{y_{t-1}} + \boldsymbol{\Upsilon} \boldsymbol{u_t} + \boldsymbol{w_t}
\end{equation}


\begin{equation}\label{StateSpace7}
\boldsymbol{z_t} = A_t \boldsymbol{y_{t}} + + \boldsymbol{\Gamma} \boldsymbol{u_t} + \boldsymbol{v_t}
\end{equation}

con $\boldsymbol{\Upsilon}$ y $\boldsymbol{\Gamma}$ de dimensión $p x r$ y $q x r$, respectivamente. 

El objetivo es tener estimaciones de $x_t$ a partir de las observaciones $y_1, y_2, ..., y_s$. Cuando $s < t$ se trata de predicción. Cuando $s = t$ se habla de filtrado y para $s > t$ de suavizado \cite{Shumway2006}.

\begin{equation}\label{StateSpace3}
\boldsymbol{y_t^s} = E\{\boldsymbol{y_t} |\boldsymbol{ z_s}\}
\end{equation}

\begin{equation}\label{StateSpace4}
\boldsymbol{P_{t}^s} = E\{\boldsymbol{(y_{t1}-y_{t1}^s)} |\boldsymbol{{(y_{t2}-y_{t2}^s)}}'\}
\end{equation}


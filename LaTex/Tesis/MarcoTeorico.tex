\chapter{Antecedentes teóricos}

En este capítulo se contextualiza de forma teórica el problema de proyección de poblaciones carcelarias, desde una perspectiva demográfica. Posteriormente se presentan, siguiendo a Shumway y Stoffer en \cite{Shumway} y \cite{Shumway2006}, los conceptos asociados al manejo de series de tiempo: modelos SARIMA, Series de tiempo estructurales y modelos estado espacio. El modelo-censal ratio se presenta en el capítulo 4, pues se le realizan ajustes para usarlo con los datos disponibles.

En las secciones dedicadas a los modelos SARIMA y a los modelos estado-espacio se mencionan los métodos de estimación y proyección usados en los paquetes seleccionados. 

\section{Proyecciones de población}

``Una estimación poblacional consiste en determinar el tamaño o las características de una población, para el momento actual o para uno anterior, en ausencia de información. Cuando se realizan un conjunto de supuestos sobre el comportamiento de los vitales hacia el futuro, hablamos de proyección, y cuando se escoge un escenario como el más probable, hablamos de pronóstico'' \cite {Swanson2012}.

Para incluir la incertidumbre en las proyecciones de población Lee(1994) considera los siguientes métodos \cite {Lee1994}: 

\begin{itemize}
	\item El enfoque de escenarios alto, medio y bajo: Asume comportamientos fijos para la fertilidad, la mortalidad y las migraciones durante el período de proyección, basado en algunos supuestos \cite {Lee1994}.
	\item Análisis estocásticos
	\begin{itemize}
		\item Análisis ex-post: consiste en evaluar el error de pronóstico en proyecciones anteriores y aplicarlo a las nuevas proyecciones \cite {Lee1994}.
		\item Simulación estocástica: Permite hacer proyecciones de población, al asignar una distribución de probabilidad a las tasas vitales (mortalidad, natalidad, migraciones) \cite {Lee1994}.
		\item Modelos estocásticos de la tasa de crecimiento: Consiste en estimar la tasa de crecimiento del total de la población; aunque permite estimar intervalos de confianza, no permite separar la proyección de las tasas vitales, ni de las franjas etarias \cite {Lee1994}.
		\item Matrices de Leslie con modelos estimados para las tasas vitales:  Al usar matrices de Leslie se estima la población por rangos etarios para un instante i, y se calcula la población en el instante i + 1 aplicando la natalidad y la mortalidad proyectadas para el período i. Puesto que las series de población carcelaria no se publican separadas por edad, no se abordará esta técnica.
	\end{itemize}
\end{itemize}

\subsection{Proyección de poblaciones pequeñas}

La proyección de áreas pequeñas es entendida como la proyección a un nivel geográfico menor al nacional. Estas proyecciones pueden incluir, departamentos, ciudades o poblaciones especiales \cite {Swanson2012}.
``Una población especial es un grupo poblacional que se encuentra restringido a un área por una medida administrativa o legislativa. Dentro de los grupos usualmente considerados se encuentran las prisiones, universidades, hospitales e instituciones militares''.
Este tipo de población puede tener una estructura etaria y de sexo, y unos vitales diferentes al resto de la población; además no suelen envejecer en el mismo lugar, lo que permite mantener una estructura etaria que no varía a través del tiempo. \cite {Swanson2012}.

\subsection{Aplicaciones nacionales e internacionales}

Las proyecciones de poblaciones carcelarias oficiales analizadas corresponden, en buena parte, a los métodos expuestos en los capítulos anteriores:  Proyecciones por escenarios, proyección de la tasa de crecimiento, modelos ARIMA para las tasas de ingreso y salida.

En Colombia, el CONPES 3828 proyectó la población carcelaria usando la tasa media de crecimiento anual (1993-2014) sin incluir una medida de incertidumbre \cite {DepartamentoNacionaldePlaneacion2015}. Este tipo de proyecciones que de forma subyacente incluyen el supuesto de un sistema estable ( que seguirá comportándose de la misma manera a través del tiempo) omiten el efecto de los cambios en las tasas de ingreso y salida del sistema carcelario, asociados a cambios en la política criminal o a cambios en la población.

El Reino Unido hasta 2015 realizaba una proyección por escenarios (alto, medio y bajo), año en el cual cambió a un modelo de proyección de la media y su incertidumbre. La incertidumbre se incluyó a través de un análisis ex-post, de la  desviación de la proyección en años anteriores \cite {Justice2014}.

El departamento de Justicia de los Estados Unidos realizó estimaciones de la población carcelaria por estado para el período 2013-2014. Las estimaciones parten del censo de prisiones 1993-2014. Estas proyecciones se puede enmarcar dentro de las proyecciones de áreas pequeñas \cite {Minton2015}.

El bureau de estadísticas e investigación del crimen en Australia proyecta las tasas de arresto y sentencia usando modelos ARIMA;  a partir de estas tasas proyecta la población carcelaria. Estas proyecciones incluyen un período de validación de tres años. Los resultados mostraban que la serie real se encuentra dentro de los intervalos de confianza de la proyección, cercano a la proyección de la media \cite{Wan1}.

Blummstein desarrolla un método de proyección basado en los componentes demográficos, tasas específicas de arresto por delito y reincidencias, a partir de estos datos proyecta el tamaño y la composición de las poblaciones \cite{Blumstein1980a}.

Con los datos libres disponibles en Colombia no se podría utilizar el enfoque presentado por Wan et al. en\cite{Wan1} ni el método de Blummstein presentado en \cite{Blumstein1980a}, pues las tasas de ingreso al sistema y sentencia no se publican. La tesis propone tres métodos de proyección para situaciones donde no se cuenta con el registro de los vitales o su equivalente en la población analizada. En el escenario de las poblaciones carcelarias en Colombia, con los datos abiertos disponibles, resulta conveniente usar las series SARIMA en los conteos individuales de población sindicada y condenada, y los modelos estado-espacio para la serie bivariada. El método Censal-ratio, para proyectar poblaciones pequeñas, permite proyectar la población carcelaria como proporción de la población total.

\section{Series de tiempo}

Una serie de tiempo es un conjunto de observaciones ${y_t}$ asociadas a un instante de tiempo t. Es usual referirse como series de tiempo, tanto a las realizaciones ${y_t}$ como a las variables aleatorias ${Y_t}$ que las generan.\cite{Brockwell2011} 

En una regresión lineal clásica, una variable $Y$ es explicada o predicha en función de un conjunto de n variables $X$. La diferencia entre el valor observado en el instante t $y_t$ y el valor predicho se suponen provenientes de un proceso aleatorio con media cero. \cite{Commandeur2007}

\begin{equation}\label{eq:regresion}
y_t = \beta_{0} + \beta_{1} x_{t1} + \beta_{2} x_{t2} ... \beta_{n} x_{tn}  + w_t
\end{equation}

También es posible representar la observación $y_t$ en función de las observaciones anteriores como:

\begin{equation}\label{eq:serierecursiva}
y_t = \beta_{0} + \beta_{1} y_1 + \beta_{2} y_2 + ... + \beta_{t-1} y_{t-1} +  w_t
\end{equation}

La variable aleatoria $Y$ en el instante i, es una combinación lineal de los valores observados de la variable aleatoria, más un error aleatorio, donde $\{w_1, w_2, ...w_t\}$ son independientes e idénticamente distribuidas (i.i.d.). \cite{Shumway} Este supuesto no suele cumplirse, razón por la cual el análisis de series de tiempo se ha desarrollado como un área particular de la estadística \cite{Brockwell2011}.

Se define un proceso como ruido blanco a la colección de variables aleatorias $z_t$, no correlacionadas, con media $0$ y varianza $\sigma^2_w$. \cite{Shumway}

Bajo un modelo de $k$ parámetros, el estimador máximo verosímil de la varianza es: 

\begin{equation}\label{varianza}
\hat {\sigma}^2_k = \frac{SSE(k)}{n} = \frac{1}{n} \sum_{t = 1}^{n}  (y_t - \hat{y_t})
\end{equation}

Para seleccionar el modelo que mejor explique los datos observados se recurre a los criterios AIC (Criterio de Información de Aikake) y BIC (Criterio de Información Bayesiano), que penalizan los modelos con mayor cantidad de parámetros y están definidos como: 

\begin{equation}\label{AIC}
AIC = ln (\hat{\sigma_k}^2) + \frac{n+2k}{n}
\end{equation}

\begin{equation}\label{AIC}
BIC = ln (\hat{\sigma_k}^2) + \frac{k (ln(n))}{n}
\end{equation}

donde $k$ es la cantidad de parámetros en el modelo y $n$ es la cantidad de observaciones \cite{Tsay2002}.  Tanto en el AIC como en el BIC se selecciona el modelo de $k$ parámetros con menor valor.  

Para medir la dependencia entre las observaciones recurrimos a la función de autocovarianza  y a la función de autocorrelación (ACF). La función de autocovarianza se define como \cite{Shumway}: 

\begin{equation}\label{covarianza}
\gamma(s,t) = cov(y_{s}, y_{t}) = E[(y_{s} - \mu_{s})(y_{t} - \mu_{t})]
\end{equation}

para todo s y t, donde $\mu = EY_t$ y k representa la cantidad de pasos entre dos observaciones. La autocovarianza mide fla dependencia lineal entre dos puntos en la misma serie en diferentes instantes. \cite{Shumway}. 

La función de autocorrelación (ACF) se define como \cite{Brockwell2011}: 

\begin{equation}\label{correlacion}
\rho (s,t) = \dfrac{\gamma(s,t)}{\sqrt{\gamma(s,s) \gamma(t,t)}} 
\end{equation}


Para poder realizar predicciones sobre el estado futuro de una serie de tiempo, es necesario suponer que el comportamiento de la serie es estable a través del tiempo, incluso con un componente aleatorio. La estacionaridad es el concepto que permite articular esta necesidad. Estamos ante un \textbf{proceso estacionario} cuando las variables $\{X_1,..., X_k\}$ tienen la misma distribución conjunta que $\{X_{h+1},...,X_{h+k}\}$, para todos los enteros $h$ y $k$ \cite{Brockwell2011}.

La estacionaridad débil se presenta cuando $E[Y_t]$ y $E[Y_t,Y_{t+h}]$ son independientes de $t$, es decir: i) Presenta media $\mu$ constante e independiente de $t$ ii) La función de autocovarianza $\gamma(h, h+k)$ depende solamente de la cantidad de pasos que separa las observaciones ($k$). En adelante, al referirse a estacionaridad, se tratará de estacionaridad debil, a menos que se indique lo contrario.\cite{Shumway}

Cuando la serie no tiene un comportamiento estacionario se puede recurrir a diferenciar la serie, este proceso al ser de uso frecuente tiene su propia notación. La diferencia $\bigtriangledown y_t = y_t - y_{t-1}$ se conoce como diferencia de primer orden. La diferencia de segundo orden se define como $\bigtriangledown^2 y_t = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2} )$.

El operador de rezago B se define como \cite{Shumway}:

\begin{equation}\label{backshift}
By_t = y_{t_1} 
\end{equation}

tal que: 
\begin{equation}\label{backshift2}
\bigtriangledown y_t = (1-B) y_t 
\end{equation}

La notación se puede extender de forma que: 

\begin{equation}\label{backshift3}
\bigtriangledown^d y_t = (1-B)^d y_t 
\end{equation}

La función de auto-correlación (ACF) de una serie de tiempo estacionaria  se define como \cite{Shumway}: 

\begin{equation}\label{autocorrelacion}
\rho (h) = \frac{\gamma (h)}{\gamma(0)}
\end{equation}

donde $\gamma (h) = cov(y_{t+h},y_t) = E[(y_{t-h} - \mu)(y_t- \mu)]$ y $\bar{y}$ denota la media del proceso.


Para estimar la ACF utilizamos la función de autocorrelación muestral definida como \cite{Shumway}: 

\begin{equation}\label{autocorrelacionmuestral}
\hat{\rho (h)} = \frac{\hat{\gamma} (h)}{\hat{\gamma} (0)} = \frac{\sum_{t = 1}^{n - h}(y_{t+h}-\bar{y})(y_t - \bar{y})}{\sum_{t = 1}^{n}(y_t-\bar{y})^2}
\end{equation}
con $ h = 0,1, ..., n-1$.

Si tenemos las variables aleatorias $X,Y,Z$ y queremos medir la correlación entre las variables $Z$ y $X$, descontando el efecto de la variable Y, recurrimos a la función de correlación parcial $\rho_{XY|Z}$, que se calcula como \cite{Shumway}: 

\begin{equation}\label{correlacionparcial}
\rho_{XY|Z} = corr\{ X-\hat{X},Y-\hat{Y}\}
\end{equation}

Al adaptar este concepto a series de tiempo tenemos que la función de autocorrelación parcial de un proceso estacionario $y_t$, $\alpha_{hh}$, con $h = 1, 2, ... , t$ se define como \cite{Shumway}: 

\begin{equation}\label{correlacionparcialestacionario1}
\alpha_{11} = corr(y_1,y_0)  = \rho(1)
\end{equation}

y 

\begin{equation}\label{correlacionparcialestacionario2}
\alpha_{hh} = corr(y_h - \hat{y_h},y_0 - \hat{y_0}), h \geq 2 
\end{equation}

donde $\hat{y_h}$ es la regresión de $y_h$ en $\{y_1,y_2, ..., y_{h-1}\}$  y $\hat{y_0}$ es la regresión de $y_0$ en $\{y_1,y_2, ..., y_{h-1}\}$.

En un proceso estacionario $y_t $ la PACF es la correlación entre $ y_t,y_{t+h} $ una vez controlado por los elementos entre ellos $ \{y_{t-h+1}, y_{t-h+2},..., y_{t-1} \} $ \cite{Shumway}.

\subsection{Modelos SARIMA}

Box \& Jenkins proponen una aproximación iterativa de  cuatro etapas para la selección de un modelo \cite{Box2013}: 

\begin{enumerate}
	\item Selección de una clase de modelos, con base en la teoría y la práctica.
	\item Identificación del modelo, donde se seleccionan un conjunto de parámetros que permitan explicar el sistema con parsimonia. 
	\item Estimación de parámetros. 
	\item Chequeo diagnóstico, para detectar fallas en el ajuste. Si se detectan fallas en el ajuste, se regresa al segundo paso. 
\end{enumerate}

Shumway en \cite{Shumway} plantea que un proceso es auto-regresivo de orden p o AR(p) si:

\begin{equation}\label{ar}
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} ... + \phi_p y_{t-p} + w_t
\end{equation}

Donde $y_t$ es estacionaria con $\mu = 0$ ;  $ \phi_1,  \phi_2,...  ,\phi_p$ son constantes  ($\phi_p \neq 0$) y los $w_t$ son independientes e idénticamente distribuidos. Cuando la $\mu \neq 0$

\begin{equation}\label{ar1}
y_t - \mu  = \phi_1 (y_{t-1} - \mu) + \phi_2 (y_{t-2}-\mu) ... + \phi_p (y_{t-p}-\mu) + w_t
\end{equation}

Que se puede representar como:

\begin{equation}\label{ar2}
y_t  = \mu(1 - \phi_1 ... - \phi_p) + \phi_1 (y_{t-1}) + \phi_2 (y_{t-2}) ... + \phi_p (y_{t-p}) + w_t
\end{equation}

También es posible utilizar el operador B para representar la serie \ref{ar} como:

\begin{equation}\label{ar3}
y_t  - \phi_1 (y_{t-1}) - \phi_2 (y_{t-2}) ... - \phi_p (y_{t-p}) =  w_t
\end{equation} 

\begin{equation}\label{ar4}
(1 - \phi_1 B - \phi_2 B^2 ... - \phi_p B^p)y_t =  w_t
\end{equation} 

o en una notación simplificada: 

\begin{equation}\label{ar5}
\phi(B)y_t =  w_t
\end{equation} 

Se considera un proceso de promedio móvil de orden q o MA(p) si:

\begin{equation}\label{ma}
y_t = \theta_1 w_{t-1} + \theta_2 w_{t-2} ... \theta_q w_{t-q} + w_t
\end{equation}

Donde $ \theta_1,  \theta_2,...  ,\theta_q$ son parámetros  ($\phi_p \neq 0$) y los $w_t$ son independientes e idénticamente distribuidos y $y_t$ tiene media cero. A diferencia de los procesos auto-regresivos, los procesos de promedio movil son estacionarios sin importar el valor de $\theta_1,  \theta_2,...  ,\theta_q$ \cite{Shumway}. Este proceso también se puede representar usando el operador de rezago al ponerlo en la forma: 

\begin{equation}\label{ma1}
y_t = (\theta_1 B + \theta_2 B^2 ... \theta_q B^q + 1) w_t
\end{equation}

o en forma simplificada: 

\begin{equation}\label{ma2}
y_t = \theta (B) w_t
\end{equation}

Un proceso se considera ARMA(p,q) si $y_t$ es estacionaria y  \cite{Shumway} : 

\begin{equation}\label{arma1}
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \theta_1 w_{t-1} + \theta_2 w_{t-2} ... \theta_q w_{t-q} + w_t
\end{equation}

Con $E(y_t) = \mu = 0$. Cuando $(\mu \neq 0)$ se puede representar como: 

\begin{equation}\label{arma2}
y_t  = \mu(1 - \phi_1 ... - \phi_p) + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \theta_1 w_{t-1} + \theta_2 w_{t-2} ... \theta_q w_{t-q} + w_t
\end{equation}

A este modelo se le conoce como ARMA(p,q). Usando las ecuaciones \ref{ar5} y  \ref{ma2} se puede representar como: 

\begin{equation}\label{arma3}
\phi(B)y_t  =  \theta (B) w_t
\end{equation}

Una técnica común para tratar con series no estacionarias consiste en diferenciarlas hasta que la serie obtenida sea ARMA(p,q) \cite{Brockwell2011}. Si la serie se diferencia $d$ veces el proceso se nota como ARIMA(p,d,q).

Un proceso ARIMA (i,1,j) resulta al considerar una serie de la forma \cite{Shumway}: 

\begin{equation}\label{ARIMA}
y_t = \alpha +  y_{t-1}
\end{equation}

Tal que el proceso $y_t - y_{t-1}$ es un proceso ARMA(p,q).

Los criterios para identificar el orden de un ARMA son \cite{Boland2011}: 

\begin{itemize}
	\item Cuando la función de autocorrelación (ACF) se reduce progresivamente, y  la función de autocorrelación parcial (PACF) no tiene picos en lags luego de p, es un proceso autoregresivo de orden p  AR(p) 
	\item Cuando la función de autocorrelación tiene un pico en el lag q, y la función de autocorrelación parcial se reduce progresivamente, es un proceso de media mmovil de orden q MA(q)
	\item Si ambas funciones se reducen gradualmente, se trata de un ARMA (p,q)
\end{itemize}

Cuando las series presentan un comportamiento estacional se recurre al proceso de diferenciación estacional de la forma: \cite{Tsay2002}

\begin{equation}\label{ARIMA}
\bigtriangledown_s y_t = y_t - y_{t-s} = (1 - B^s) y_t
\end{equation}

Un proceso autoregresivo de orden $P$, promedio móvil de orden $Q$ estacional $ARMA(P,Q)_s$, donde s corresponde a la periodicidad de la serie se define como \cite{Shumway}: 

\begin{equation}\label{sarma1}
\Phi_P  (B^s) y_t  =  \Theta_Q (B^s) w_t
\end{equation}

donde $\Phi_P (B^s) = 1- \Phi_1 B^s - \Phi_2 B^{2s}- ... - \Phi_P B^{Ps}$ se denomina operador estacional autoregresivo de orden P y $\Theta_Q (B^s) = 1 + \Theta_1 B^s + \Theta_2 B^{2s} + ... + \Theta_Q B^{Qs}$ se denomina operador estacional promedio movil de orden Q \cite{Shumway}.

Un proceso SARIMA(p,d,q)(P,D,Q) se define como: 

\begin{equation}\label{sarma1}
\Phi_P  (B^s) \phi_p (B^s) \bigtriangledown^D \bigtriangledown^d y_t  =  \Theta_Q (B^s) \theta_q (B) w_t
\end{equation}

donde $w_t$ es un proceso gausiano de ruido blanco.

El paquete $astsa$ se ha utilizado para estimar los modelos ARIMA en el capitulo 3. La función $sarima$ del paquete $astsa$ es un manejador de la función $arima$ del paquete base. \cite{Stoffer2014}. La función $arima$ utiliza la función $optim$ para encontrar estimadores de máxima verosimilitud.\cite{RCoreTeam2017}.

El paquete $astsa$ se ha utilizado para realizar los pronósticos en el capitulo 3. La función $sarima.for$ del paquete $astsa$ es un manejador de la función $predict.Arima$ del paquete base. \cite{Stoffer2014}. La función $predict.Arima$ utiliza la función $KalmanForecast$. En la siguiente sección se hablará en detalle del filtro de Kalman. \cite{RCoreTeam2017}

Una vez se ha realizado la estimación del modelo, se analiza el comportamiento del error. Para descartar autocorrelación en el error se utiliza la estadística Q de Ljung, definida como:

\begin{equation}\label{ljung}
Q = n(n+2) \sum_{n=1}^{H} \frac{ \hat {\rho_{e}}^2(h)}{n-1}
\end{equation}

donde $H$ es el rezago analizado, $\hat{\rho_{e}}(h)$  es la autocorrelación estimada del error en el rezago h. La hipótesis nula es que los errores son no correlacionados. Bajo la hipótesis nula $Q \sim \chi_{H}^2$. \cite{Ljung1978} \cite{Shumway}

\subsection{Modelos de Series de Tiempo Estructurales}

En economía y ciencias sociales hay fenomenos que se pueden descomponer en dos componentes básicos, un comportamiento estable de largo plazo (tendencia) y un comportamiento que se repite cada cierto tiempo, usualmente anual (estacionalidad). Estos fenomenos se pueden representar de la forma: \cite{harvey1990forecasting}

\begin{equation}\label{StructuralTimeSeries1}
observado = tendencia + estacionalidad + irregularidad.  
\end{equation}

o en forma multiplicativa 

\begin{equation}\label{StructuralTimeSerie2}
observado = tendencia \times estacionalidad \times irregularidad.  
\end{equation}

La ecuación \ref{StructuralTimeSerie2} puede ser llevada a la forma \ref{StructuralTimeSeries1} al tomar logaritmo en cada lado de la ecuación. 

En los modelos de series de tiempo estructurales, al representar en componentes de tendencia, estacionalidad y ciclo, estos tienen una interpretación directa. Más que representar el proceso que genera los datos, separa la serie en componentes con una interpretación directa de cada uno. En los modelos de series de tiempo estructurales, los componentes no son fijos, sino que se consideran generados por impactos aleatorios. \cite{harvey1990forecasting}

\subsection{Modelos Estado-Espacio}

En un modelo estado espacio una serie de tiempo (multiple) observada  $\ z_1, ... , z_t $\ depende de un estado $\ y_t $\ , posiblemente no observado, que se comporta siguiendo un proceso estocástico. La relación entre $\ y_t $\ y $\ z_t $\ está dada por la \textit{ecuación de medida}: \cite{Lutkepohl2005a}

\begin{equation}\label{EstadoEspacio}
\ z_t = H_t y_t + v_t
\end{equation}

donde $\ H_t $\ es una matriz que puede o no depender del tiempo $\ t $\ y $\ v_t $\ es el error de observación, que se asume usualmente como un proceso de ruido. El vector de estado es generado como: 

\begin{equation}\label{EstadoEspacio2}
y_t = B_{t-1} y_{t-1} + w_{t-1}
\end{equation}

La matriz $\ B_t $\ es una matriz de coeficientes que puede depender de $\ t $\, y $\ w_t $\ es un proceso de ruido. \cite{Lutkepohl2005a}

Una forma de representar los modelos de series de tiempo estrucuturales es a través de los modelos Estado-Espacio, que además permiten representar otros modelos como Regresión Lineal o Modelos ARIMA. \cite{Durbin2012}

Los modelos Estado-Espacio o Modelos Lineales Dinamicos (DLM), en su forma más simple emplean un vector autoregresivo de la forma \cite{Shumway2006}: 

\begin{equation}\label{StateSpace1}
\boldsymbol{y_t} = \Phi \boldsymbol{y_{t-1}} + \boldsymbol{w_t}
\end{equation}

donde $\boldsymbol{y_t}$ es un vector de $p$ componentes. $ \boldsymbol w_t$ es un vector de $p \times 1$ variables aleatorias independientes e idénticamente distribuidas normalmente con media cero y matriz de covarianzas $ \boldsymbol Q$. Asumimos que el proceso es normal con un vector $ \boldsymbol {y_0}$ con media $ \boldsymbol {\mu_0}$ y matriz de covarianzas $ \boldsymbol \sum$. \cite{Shumway2006}


\begin{equation}\label{StateSpace2}
\boldsymbol{z_t} = A_t \boldsymbol{y_{t}} + \boldsymbol{v_t}
\end{equation}

donde $A_t$ es de tamaño $q \times p$. $\boldsymbol {v_t}$ se asume ruido blanco, con  matriz de covarianzas  $R$. Se suponen $\boldsymbol {w_t}$ y $\boldsymbol {v_t}$ no correlacionados. 

Holmes en \cite{Holmes2018a} demonima a los modelos de esta forma modelos Estado Espacio Multivariados Auto Regresivos (MARSS). Sobre esta estrucuctura de modelos se realizan las aplicaciones en el capítulo 5.  

Variables exógenas o inputs fijos pueden ingresar en los estados o las observaciones. En este caso tendremos un vector $u_t$ de tamaño $r \times 1$: 

\begin{equation}\label{StateSpace6}
\boldsymbol{y_t} = \Phi \boldsymbol{y_{t-1}} + \boldsymbol{\Upsilon} \boldsymbol{u_t} + \boldsymbol{w_t}
\end{equation}

\begin{equation}\label{StateSpace7}
\boldsymbol{z_t} = A_t \boldsymbol{y_{t}} + + \boldsymbol{\Gamma} \boldsymbol{u_t} + \boldsymbol{v_t}
\end{equation}

con $\boldsymbol{\Upsilon}$ y $\boldsymbol{\Gamma}$ de dimensión $p \times r$ y $q \times r$, respectivamente. 

El objetivo es tener estimaciones de $y_t$ a partir de las observaciones $z_1, z_2, ...,z_s$. Cuando $s < t$ se trata de predicción. Cuando $s = t$ se habla de filtrado y para $s > t$ de suavizado \cite{Shumway2006}.

\begin{equation}\label{StateSpace3}
\boldsymbol{y_t^s} = E\{\boldsymbol{y_t} |\boldsymbol{ z_s}\}
\end{equation}

\begin{equation}\label{StateSpace4}
\boldsymbol{P_{t}^s} = E\{\boldsymbol{(y_{t1}-y_{t1}^s)} |\boldsymbol{{(y_{t2}-y_{t2}^s)}}'\}
\end{equation}

Con valores iniciales $\boldsymbol{x_0^0} =\boldsymbol{\mu_0}$ y ${P_0^0 = \Sigma_0}$ para $t = 1, 2, ... , n$

\begin{equation}\label{StateSpace9}
\boldsymbol{y_t^{t-1}} = \Phi \boldsymbol {y_{t-1}^{t-1}} + \Gamma \boldsymbol{u_t}
\end{equation}

\begin{equation}\label{StateSpace8}
\boldsymbol{P_t^{t-1}} = \Phi \boldsymbol {P_{t-1}^{t-1}} \Phi' + \boldsymbol{Q} 
\end{equation}

con 

\begin{equation}\label{StateSpace10}
\boldsymbol{y_t^{t}} = \boldsymbol {y_{t}^{t-1}} + K_t(\boldsymbol {z_t} +  {A_t} \boldsymbol{y_t^{t-1}} - \Gamma \boldsymbol{u_t}) 
\end{equation}

\begin{equation}\label{StateSpace11}
\boldsymbol{P_t^{t}} = [I - K_t A_t] {P_{t}^{t-1}}
\end{equation}

donde
 
 \begin{equation}\label{StateSpace12}
 \boldsymbol{K_t} = P_{t}^{t-1} A_t'[A_t P_{t}^{t-1} A_t' + R]^{-1}
 \end{equation}

$K_t$ se conoce como ganancia de Kalman. Predicciones para t > n se realizan con \ref{StateSpace9}, con condiciones iniciales $y_n^n$ y $P_n^n$.


